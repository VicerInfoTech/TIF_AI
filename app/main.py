"""FastAPI application for SQL Insight Agent."""

from __future__ import annotations

# pylint: disable=duplicate-code
import re
from datetime import datetime
from os import getenv
from pathlib import Path
from time import perf_counter
from typing import Any, Dict, List

from fastapi import FastAPI, HTTPException, status
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, RedirectResponse
from fastapi.middleware.cors import CORSMiddleware

from app.models import (
    QueryRequest,
    QueryResponse,
    ExecutionMetadata,
    HealthResponse,
    SchemaEmbeddingRequest,
    SchemaEmbeddingResponse,
    SchemaPipelineRequest,
    SchemaPipelineResponse,
    SchemaPipelineReport,
    ExtractionStageSummary,
    DocumentationStageSummary,
    EmbeddingStageSummary,
)

from app.agent.chain import (
    agent_context,
    default_collection_name,
    get_available_providers,
    get_cached_agent,
    get_cached_agent_with_context,
    get_collected_tables,
    parse_structured_response,
    summarize_query_results,
)
from app.user_db_config_loader import get_user_database_settings, PROJECT_ROOT
from db.model import DatabaseConfig
from db.database_manager import create_metadata_tables, get_project_db_connection_string, get_session
from sqlalchemy.exc import SQLAlchemyError
from app.core import query_executor, result_formatter, sql_validator
from app.schema_pipeline import SchemaPipelineOrchestrator
from app.schema_pipeline.embedding_pipeline import (
    SchemaEmbeddingPipeline,
    SchemaEmbeddingSettings,
)
from app.utils.logger import setup_logging
from db.database_manager import get_session
from db.model import DatabaseConfig
from sqlalchemy.exc import IntegrityError, SQLAlchemyError
from db.conversation_memory import (
    store_query_context,
    get_session_summary,
    update_or_create_session_summary,
)

# Initialize logging
logger = setup_logging(__name__)

# Create FastAPI app
app = FastAPI(
    title="SQL Insight Agent",
    description="Natural Language to SQL query agent powered by LangChain with provider fallback",
    version="1.0.0",
)

# Dev-only static UI (chat page)
static_dir = Path(__file__).parent / "static"
if static_dir.exists():
    # mount under /static so files are available -- dev convenience only
    app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")


@app.get("/chat")
async def chat_ui():
    """Dev UI: serve a small chat HTML page for manual testing.

    This route intentionally returns the static chat UI that posts to /query.
    It should be considered a development convenience and not a production feature.
    """
    f = static_dir / "chat.html"
    if f.exists():
        return FileResponse(f)
    # if static not present, redirect to OpenAPI docs as fallback
    return RedirectResponse(url="/docs")

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Models moved to `app.models` for reusability and readability

def _sanitize_sql(sql_text: str) -> str:
    """Remove formatting fences and whitespace from the agent's SQL output."""

    if not sql_text:
        return ""
    cleaned = sql_text.strip()

    code_blocks = re.findall(r"```(?:sql)?\s*([\s\S]*?)```", cleaned, flags=re.IGNORECASE)
    if code_blocks:
        cleaned = code_blocks[-1].strip()

    if cleaned.lower().startswith("sql"):
        cleaned = cleaned[3:].lstrip(" :\n")

    select_match = re.search(r"select", cleaned, flags=re.IGNORECASE)
    if select_match:
        cleaned = cleaned[select_match.start():]

    return cleaned.strip()


def _extract_agent_output(agent_result: Any) -> str:
    """Normalize the agent response into a plain string."""

    def _stringify_segments(content: Any) -> str:
        if isinstance(content, str):
            return content
        if isinstance(content, list):
            parts = []
            for item in content:
                if isinstance(item, str):
                    parts.append(item)
                elif isinstance(item, dict):
                    text_value = item.get("text")
                    if text_value:
                        parts.append(str(text_value))
                else:
                    text_attr = getattr(item, "text", None)
                    if text_attr:
                        parts.append(str(text_attr))
                    else:
                        parts.append(str(item))
            return "\n".join(part for part in parts if part)
        return str(content)

    if isinstance(agent_result, dict):
        messages = agent_result.get("messages")
        if isinstance(messages, list) and messages:
            final_message = messages[-1]
            content = getattr(final_message, "content", None)
            if content is not None:
                return _stringify_segments(content)
        for key in ("output", "content", "answer"):
            value = agent_result.get(key)
            if value:
                return _stringify_segments(value)
    return _stringify_segments(agent_result)


# Endpoints
@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint."""
    logger.debug("Health check requested")
    return HealthResponse()


@app.post("/query", response_model=QueryResponse)
async def execute_query(request: QueryRequest) -> QueryResponse:
    """Execute a natural language SQL query.

    Args:
        request: QueryRequest with query, db_flag, and output_format

    Returns:
        QueryResponse with status, SQL, validation result, and formatted data

    Raises:
        HTTPException: If execution fails or database is unavailable
    """
    try:
        logger.info(
            "Received query request: query=%s, db_flag=%s, format=%s",
            request.query,
            request.db_flag,
            request.output_format,
        )
        logger.debug(
            "Conversation identifiers user_id=%s session_id=%s",
            request.user_id,
            request.session_id,
        )

        try:
            db_settings = get_user_database_settings(request.db_flag)
        except KeyError as exc:  # pragma: no cover - handled explicitly
            logger.error("Configuration error loading db_flag=%s: %s", request.db_flag, str(exc))
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Unknown database: {str(exc)}",
            ) from exc
        
        db_config = db_settings.model_dump()
        collection_name = default_collection_name(request.db_flag)

        providers = get_available_providers()

        logger.debug("Provider order determined by environment: %s", providers)
        
        agent_output: Dict[str, Any] | None = None
        selected_tables: List[str] = []
        last_error: Exception | None = None
        successful_provider: str | None = None
        follow_up_questions: List[str] | None = None

        # Use context-aware agent if user_id and session_id are provided
        for provider in providers:
            try:
                if request.user_id and request.session_id:
                    # Use conversation-aware agent
                    agent = get_cached_agent_with_context(
                        provider,
                        request.db_flag,
                        user_id=request.user_id,
                        session_id=request.session_id,
                    )
                    logger.debug(f"Using context-aware agent for user={request.user_id}, session={request.session_id}")
                else:
                    # Use stateless agent (backward compatible)
                    agent = get_cached_agent(provider, request.db_flag)
                    logger.debug("Using stateless agent (no user/session context)")

                with agent_context(
                    request.db_flag,
                    collection_name,
                    user_id=request.user_id,
                    session_id=request.session_id,
                ):
                    agent_output = agent.invoke(
                        {
                            "messages": [
                                {
                                    "role": "user",
                                    "content": request.query,
                                }
                            ]
                        }
                    )
                    selected_tables = get_collected_tables()
                logger.info("Generated SQL using provider=%s", provider)
                successful_provider = provider
                break
            except Exception as exc:  # noqa: BLE001
                last_error = exc
                logger.exception("Provider %s failed during SQL generation", provider)

        if agent_output is None:
            detail = (
                f"LLM providers unavailable: {last_error}" if last_error else "All LLM providers failed"
            )
            raise HTTPException(
                status_code=status.HTTP_502_BAD_GATEWAY,
                detail=detail,
            )

        structured_llm_response = parse_structured_response(agent_output)
        contextual_insights: str | None = None
        if structured_llm_response:
            raw_output = structured_llm_response.sql_query
            contextual_insights = structured_llm_response.query_context
            # Preserve empty list (do not coerce to None) for API clients that expect an array
            follow_up_questions = structured_llm_response.follow_up_questions
        else:
            raw_output = _extract_agent_output(agent_output)
        sql_generated = _sanitize_sql(raw_output)
        logger.info("Generated SQL (raw): %s", sql_generated)
        if not sql_generated:
            logger.error("Agent returned empty SQL output")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Agent returned empty SQL output",
            )

        validation_result = sql_validator.validate_sql(sql_generated)
        validation_ok = validation_result.get("valid", False)
        logger.info("Validated SQL: %s (valid=%s, reason=%s)", sql_generated, validation_ok, validation_result.get("reason"))
        if not validation_ok:
            logger.warning("SQL validation failed: %s", validation_result.get("reason"))
            return QueryResponse(
                status="error",
                sql=sql_generated,
                validation_passed=False,
                data=None,
                error=validation_result.get("reason"),
                selected_tables=selected_tables or None,
                keyword_matches=None,
                follow_up_questions=follow_up_questions,
                metadata=ExecutionMetadata(
                    execution_time_ms=None,
                    total_rows=None,
                    retry_count=0,
                ),
                token_usage=None,
            )

        exec_start = perf_counter()
        execution = query_executor.execute_query(sql_generated, db_config)
        elapsed_ms = (perf_counter() - exec_start) * 1000
        if not execution.get("success"):
            logger.error("SQL execution failed: %s", execution.get("error"))
            return QueryResponse(
                status="error",
                sql=sql_generated,
                validation_passed=True,
                data=None,
                error=execution.get("error"),
                selected_tables=selected_tables or None,
                keyword_matches=None,
                follow_up_questions=follow_up_questions,
                metadata=ExecutionMetadata(
                    execution_time_ms=elapsed_ms,
                    total_rows=None,
                    retry_count=0,
                ),
                token_usage=None,
            )

        dataframe = execution.get("dataframe")
        formatted = result_formatter.format_results(
            dataframe=dataframe,
            sql=sql_generated,
            output_format=request.output_format,
            execution_time_ms=elapsed_ms,
        )

        if formatted.get("status") != "success":
            logger.error("Result formatting failed: %s", formatted.get("message"))
            return QueryResponse(
                status="error",
                sql=sql_generated,
                validation_passed=True,
                data=None,
                error=formatted.get("message", "Failed to format results"),
                selected_tables=selected_tables or None,
                keyword_matches=None,
                follow_up_questions=follow_up_questions,
                metadata=ExecutionMetadata(
                    execution_time_ms=elapsed_ms,
                    total_rows=None,
                    retry_count=0,
                ),
                token_usage=None,
            )

        total_rows_raw = formatted.get("data", {}).get("row_count") if formatted.get("data") else None
        total_rows: int | None = None
        if total_rows_raw is not None:
            try:
                # Handle floats, strings, numpy types, etc.
                total_rows_int = int(float(total_rows_raw))
                if total_rows_int >= 0:
                    total_rows = total_rows_int
            except (TypeError, ValueError):
                logger.debug("Unable to coerce row_count=%r (%s) to int", total_rows_raw, type(total_rows_raw))
                total_rows = None

        logger.info(
            "Query execution completed: rows=%s elapsed_ms=%.1f",
            total_rows,
            elapsed_ms,
        )

        result_data = formatted.get("data") or {}
        natural_summary = None
        if successful_provider:
            describe_text = result_data.get("describe_text", "")
            raw_json = result_data.get("raw_json", "")
            natural_summary = summarize_query_results(successful_provider, describe_text, raw_json)

        # Store query context in conversation history if user_id and session_id provided
        if request.user_id and request.session_id:
            try:
                store_query_context(
                    user_id=request.user_id,
                    session_id=request.session_id,
                    db_flag=request.db_flag,
                    query_text=request.query,
                    sql_generated=sql_generated,
                    tables_used=selected_tables or [],
                    follow_up_questions=follow_up_questions or [],
                    contextual_insights=contextual_insights,
                    execution_time=elapsed_ms / 1000.0 if elapsed_ms else None,
                )
                # Update session summary with new context
                update_or_create_session_summary(
                    user_id=request.user_id,
                    session_id=request.session_id,
                    db_flag=request.db_flag,
                )
                session_summary = get_session_summary(
                    user_id=request.user_id,
                    session_id=request.session_id,
                    db_flag=request.db_flag,
                )
                if session_summary:
                    logger.debug(
                        "Session summary updated for user=%s, session=%s: %s",
                        request.user_id,
                        request.session_id,
                        session_summary.get("summary"),
                    )
                logger.debug(f"Stored query context for user={request.user_id}, session={request.session_id}")
            except Exception as exc:
                logger.warning(f"Failed to store conversation history: {exc}")
        else:
            logger.debug(
                "Skipping conversation persistence (missing identifiers) user_id=%s session_id=%s",
                request.user_id,
                request.session_id,
            )

        return QueryResponse(
            status="success",
            sql=sql_generated,
            validation_passed=True,
            data=formatted.get("data"),
            error=None,
            selected_tables=selected_tables or None,
            keyword_matches=None,
            follow_up_questions=follow_up_questions,
            metadata=ExecutionMetadata(
                execution_time_ms=elapsed_ms,
                total_rows=total_rows,
                retry_count=0,
            ),
            natural_summary=natural_summary,
            token_usage=None,
        )

    except ValueError as e:
        logger.error("Validation error: %s", str(e))
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Invalid request: {str(e)}",
        ) from e
    except Exception as e:
        logger.exception("Unexpected error during query execution: %s", str(e))
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Internal server error: {str(e)}",
        ) from e


@app.post("/schemas/embeddings", response_model=SchemaEmbeddingResponse)
async def generate_schema_embeddings(request: SchemaEmbeddingRequest) -> SchemaEmbeddingResponse:
    """Convert every schema YAML into embeddings stored in Postgres."""

    logger.info("Generating schema embeddings for db_flag=%s", request.db_flag)
    connection_string = getenv("POSTGRES_CONNECTION_STRING")
    if not connection_string:
        logger.error("Postgres connection string missing for embeddings")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Missing POSTGRES_CONNECTION_STRING",
        )

    try:
        settings = SchemaEmbeddingSettings(
            schema_root=SchemaEmbeddingPipeline.DEFAULT_SCHEMA_ROOT,
            minimal_output_root=SchemaEmbeddingPipeline.DEFAULT_OUTPUT_ROOT,
            collection_name=request.collection_name,
        )
        pipeline = SchemaEmbeddingPipeline(
            db_flag=request.db_flag,
            connection_string=connection_string,
            settings=settings,
        )
        result = pipeline.run()
        output_directory = pipeline.settings.minimal_output_root / request.db_flag

        message = (
            "Embeddings stored successfully"
            if result.document_chunks > 0
            else "No schema files were processed"
        )

        return SchemaEmbeddingResponse(
            db_flag=request.db_flag,
            output_directory=str(output_directory),
            processed_files=[path.name for path in result.minimal_files],
            message=message,
        )
    except HTTPException:
        raise
    except Exception as error:
        logger.exception("Schema embedding pipeline failed: %s", error)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Schema embedding pipeline failed: {error}",
        ) from error


@app.post("/schemas/enroll", response_model=SchemaPipelineResponse)
async def enroll_database(request: SchemaPipelineRequest) -> SchemaPipelineResponse:
    """Enroll and extract a database schema, run documentation and embeddings."""
    logger.info("Running schema pipeline for db_flag=%s", request.db_flag)
    # POSTGRES_CONNECTION_STRING is now handled internally by the orchestrator

    try:
        project_connection = get_project_db_connection_string()
        create_metadata_tables(project_connection)
        db_row = _fetch_or_create_database_config(request, project_connection)
    except SQLAlchemyError as err:
        logger.error("DatabaseConfig check/insert failed: %s", err)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"DatabaseConfig check/insert failed: {err}",
        )

    if db_row.schema_extracted and not request.incremental_documentation:
        logger.info("Schema already extracted for db_flag=%s and incremental=False. Skipping.", request.db_flag)
        extraction_output = PROJECT_ROOT / "database_schemas" / request.db_flag / "schema"
        extraction_summary = ExtractionStageSummary(
            status="success",
            output_directory=str(extraction_output),
            tables_exported=0,
            message="Database already enrolled and schema extraction is up to date",
        )
        documentation_stage = DocumentationStageSummary(
            status="skipped",
            tables_total=0,
            documented=0,
            failed=0,
            message="Documentation skipped because schema already exists",
        )
        embeddings_stage = EmbeddingStageSummary(
            status="skipped",
            minimal_files=0,
            document_chunks=0,
            output_directory=str(SchemaEmbeddingPipeline.DEFAULT_OUTPUT_ROOT / request.db_flag),
            message="Embedding skipped because schema already exists",
        )
        report = _build_pipeline_report(extraction_summary, documentation_stage, embeddings_stage)
        return SchemaPipelineResponse(
            db_flag=request.db_flag,
            extraction=extraction_summary,
            documentation=documentation_stage,
            embeddings=embeddings_stage,
            report=report,
        )
    
    if db_row.schema_extracted:
        logger.info("Database %s already enrolled. Proceeding with update/refresh (incremental=True).", request.db_flag)

    # Now run the pipeline as before
    try:
        orchestrator = SchemaPipelineOrchestrator(
            request.db_flag,
            include_schemas=request.include_schemas,
            exclude_schemas=request.exclude_schemas,
            run_documentation=request.run_documentation,
            incremental_documentation=request.incremental_documentation,
            run_embeddings=request.run_embeddings,
        )
        outcome = orchestrator.run()

        extraction_summary = ExtractionStageSummary(
            status="success",
            output_directory=str(outcome.extraction_output),
            tables_exported=outcome.tables_exported,
            message="Schema extraction completed",
        )

        logger.info("Schema extraction completed: tables_exported=%d", outcome.tables_exported)
        
        if request.run_documentation:
            doc_summary = outcome.documentation_summary
            if doc_summary is None:
                documentation_stage = DocumentationStageSummary(
                    status="failed",
                    tables_total=0,
                    documented=0,
                    failed=0,
                    message="Documentation stage did not produce a summary",
                )
            else:
                documentation_stage = DocumentationStageSummary(
                    status="success",
                    tables_total=doc_summary.tables_total,
                    documented=doc_summary.documented,
                    failed=doc_summary.failed,
                    message="Documentation completed",
                )
        else:
            documentation_stage = DocumentationStageSummary(
                status="skipped",
                tables_total=0,
                documented=0,
                failed=0,
                message="Documentation stage was skipped",
            )

        embeddings_output_dir = SchemaEmbeddingPipeline.DEFAULT_OUTPUT_ROOT / request.db_flag
        if request.run_embeddings:
            embedding_result = outcome.embedding_result
            if embedding_result is None:
                embeddings_stage = EmbeddingStageSummary(
                    status="failed",
                    minimal_files=0,
                    document_chunks=0,
                    output_directory=str(embeddings_output_dir),
                    message="Embedding stage did not produce results",
                )
            else:
                embeddings_stage = EmbeddingStageSummary(
                    status="success",
                    minimal_files=len(embedding_result.minimal_files),
                    document_chunks=embedding_result.document_chunks,
                    output_directory=str(embeddings_output_dir),
                    message="Embedding stage completed",
                )
        else:
            embeddings_stage = EmbeddingStageSummary(
                status="skipped",
                minimal_files=0,
                document_chunks=0,
                output_directory=str(embeddings_output_dir),
                message="Embedding stage was skipped",
            )

            _mark_schema_extracted(request.db_flag)

        report = _build_pipeline_report(extraction_summary, documentation_stage, embeddings_stage)
        return SchemaPipelineResponse(
            db_flag=request.db_flag,
            extraction=extraction_summary,
            documentation=documentation_stage,
            embeddings=embeddings_stage,
            report=report,
        )
    except HTTPException:
        raise
    except Exception as error:
        logger.exception("Schema pipeline failed: %s", error)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Schema pipeline failed: {error}",
        ) from error


def _fetch_or_create_database_config(request: SchemaPipelineRequest, project_connection: str) -> DatabaseConfig:
    session = get_session(project_connection)
    try:
        db_row = session.query(DatabaseConfig).filter_by(db_flag=request.db_flag).first()
        if db_row:
            return db_row

        db_row = DatabaseConfig(
            db_flag=request.db_flag,
            db_type=request.db_type,
            connection_string=request.connection_string,
            description=request.description,
            intro_template=request.intro_template,
            exclude_column_matches=request.exclude_column_matches,
            # Set defaults internally for removed fields
            max_rows=10000,
            query_timeout=30,
        )
        session.add(db_row)
        try:
            session.commit()
            session.refresh(db_row)
            logger.info("Inserted new DatabaseConfig for db_flag=%s", request.db_flag)
        except IntegrityError:
            session.rollback()
            db_row = session.query(DatabaseConfig).filter_by(db_flag=request.db_flag).first()
            if not db_row:
                raise
        return db_row
    finally:
        session.close()


def _mark_schema_extracted(db_flag: str) -> None:
    session = get_session(get_project_db_connection_string())
    try:
        db_row = session.query(DatabaseConfig).filter_by(db_flag=db_flag).first()
        if not db_row:
            return
        db_row.schema_extracted = True
        db_row.schema_extraction_date = datetime.utcnow()
        session.commit()
    finally:
        session.close()


def _build_pipeline_report(
    extraction_summary: ExtractionStageSummary,
    documentation_stage: DocumentationStageSummary,
    embeddings_stage: EmbeddingStageSummary,
) -> SchemaPipelineReport:
    documentation_skipped = max(
        0,
        documentation_stage.tables_total - documentation_stage.documented - documentation_stage.failed,
    )
    return SchemaPipelineReport(
        extracted_files=extraction_summary.tables_exported,
        documentation_tables_total=documentation_stage.tables_total,
        documentation_documented=documentation_stage.documented,
        documentation_failed=documentation_stage.failed,
        documentation_skipped=documentation_skipped,
        embeddings_minimal_files=embeddings_stage.minimal_files,
        embeddings_document_chunks=embeddings_stage.document_chunks,
    )


@app.get("/")
async def root():
    """Root endpoint with API documentation link."""
    return {
        "message": "SQL Insight Agent API",
        "docs": "/docs",
        "health": "/health",
        "endpoints": {
            "POST /query": "Execute natural language SQL query",
            "POST /schemas/embeddings": "Convert schema YAML definitions to embeddings",
            "POST /schemas/enroll": "Enroll a database, extract schema, document, and embed",
            "GET /health": "Health check",
        },
    }
    
if __name__ == "__main__":
    import uvicorn

    logger.info("Starting SQL Insight Agent API server")
    uvicorn.run(
        "app.main:app",
        host="127.0.0.1",
        port=8000,
        reload=False,
        log_level="info",
    )
